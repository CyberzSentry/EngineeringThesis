% !TeX spellcheck = en_GB
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                        %
%    Engineer thesis LaTeX template      % 
%                                        %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\documentclass[a4paper,twoside,12pt]{book}
\usepackage[utf8]{inputenc}                                      
\usepackage[T1]{fontenc}  
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage[polish,british]{babel} 
\usepackage{indentfirst}
\usepackage{lmodern}
\usepackage{graphicx} 
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage[page]{appendix} 
\usepackage{float}

\usepackage{setspace}
\onehalfspacing


\frenchspacing

\usepackage{listings}
\lstset{
	language={},
	basicstyle=\ttfamily,
	keywordstyle=\lst@ifdisplaystyle\color{blue}\fi,
	commentstyle=\color{gray}
}

%%%%%%%%%

 

%%%%%%%%%%%% FANCY HEADERS %%%%%%%%%%%%%%%

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LO]{\nouppercase{\it\rightmark}}
\fancyhead[RE]{\nouppercase{\it\leftmark}}
\fancyhead[LE,RO]{\it\thepage}


\fancypagestyle{onlyPageNumbers}{%
   \fancyhf{} 
   \fancyhead[LE,RO]{\it\thepage}
}

\fancypagestyle{PageNumbersChapterTitles}{%
   \fancyhf{} 
   \fancyhead[LO]{\nouppercase{\it\rightmark}}
   \fancyhead[RE]{\nouppercase{\it\leftmark}}
   \fancyhead[LE,RO]{\it\thepage}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%
% listings 
\usepackage{listings}
\lstset{%
language=C++,%
commentstyle=\textit,%
identifierstyle=\textsf,%
keywordstyle=\sffamily\bfseries, %\texttt, %
%captionpos=b,%
tabsize=3,%
frame=lines,%
numbers=left,%
numberstyle=\tiny,%
numbersep=5pt,%
breaklines=true,%
morekeywords={descriptor_gaussian,descriptor,partition,fcm_possibilistic,dataset,my_exception,exception,std,vector},%
escapeinside={@*}{*@},%
%texcl=true, % wylacza tryb verbatim w komentarzach jednolinijkowych
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%% TODO LIST GENERATOR %%%%%%%%%

\usepackage{color}
\definecolor{brickred}      {cmyk}{0   , 0.89, 0.94, 0.28}

\makeatletter \newcommand \kslistofremarks{\section*{Remarks} \@starttoc{rks}}
  \newcommand\l@uwagas[2]
    {\par\noindent \textbf{#2:} %\parbox{10cm}
{#1}\par} \makeatother


\newcommand{\remark}[1]{%
{%\marginpar{\textdbend}
{\color{brickred}{[#1]}}}%
\addcontentsline{rks}{uwagas}{\protect{#1}}%
}

%%%%%%%%%%%%%% END OF TODO LIST GENERATOR %%%%%%%%%%% 

% some issues...

\newcounter{PagesWithoutNumbers}

\newcommand{\hcancel}[1]{%
    \tikz[baseline=(tocancel.base)]{
        \node[inner sep=0pt,outer sep=0pt] (tocancel) {#1};
        \draw[red] (tocancel.south west) -- (tocancel.north east);
    }%
}%

\newcommand{\MonthName}{%
  \ifcase\the\month
  \or January% 1
  \or February% 2
  \or March% 3
  \or April% 4
  \or May% 5
  \or June% 6
  \or July% 7
  \or August% 8
  \or September% 9
  \or October% 10
  \or November% 11
  \or December% 12
  \fi}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Helvetica font macros for the title page:
\newcommand{\headerfont}{\fontfamily{phv}\fontsize{18}{18}\bfseries\scshape\selectfont}
\newcommand{\titlefont}{\fontfamily{phv}\fontsize{18}{18}\selectfont}
\newcommand{\otherfont}{\fontfamily{phv}\fontsize{14}{14}\selectfont}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\Author}{Maksym Brzęczek}
\newcommand{\Supervisor}{Błażej Adamczyk, DSc PhD}
\newcommand{\Consultant}{Michał Kawulok, PhD}
\newcommand{\Title}{Design and implementation of natural language sensitive data extraction tool}
\newcommand{\Polsl}{Silesian University of Technology}
\newcommand{\Faculty}{Faculty of Automatic Control, Electronics and Computer Science}


\begin{document} 
	
%%%%%%%%%%%%%%%%%%  Title page %%%%%%%%%%%%%%%%%%% 
\pagestyle{empty}
{
	\newgeometry{top=2.5cm,%
	             bottom=2.5cm,%
	             left=3cm,
	             right=2.5cm}
	\sffamily
	\rule{0cm}{0cm}
	
	\begin{center}
	\includegraphics[width=29mm]{polsl}
	\end{center} 
	\vspace{1cm}
	\begin{center}
	\headerfont \Polsl
	\end{center}
	\begin{center}
	\headerfont \Faculty
	\end{center}
	\vfill
	\begin{center}
	\titlefont Engineer  thesis
	\end{center}
	\vfill
	
	\begin{center}
	\otherfont \Title\par
	\end{center}
	
	\vfill
	
	\vfill
	 
	\noindent\vbox
	{
		\hbox{\otherfont author: \Author}
		\vspace{12pt}
		\hbox{\otherfont supervisor: \Supervisor}
		\vspace{12pt}
		\hbox{\otherfont consultant: \Consultant}
	}
	\vfill 
 
   \begin{center}
   \otherfont Gliwice,  \MonthName\ \the\year
   \end{center}	
	\restoregeometry
}
  

\cleardoublepage
 

\rmfamily
\normalfont


%%%%%%%%%%%% statements required by law and Dean's office %%%%%%%%%%
\cleardoublepage

\begin{flushright}
załącznik nr 2 do zarz. nr 97/08/09 
\end{flushright}

\vfill  

\begin{center}
\Large\bfseries Oświadczenie
\end{center}

\vfill

Wyrażam  zgodę / Nie wyrażam zgody*  na  udostępnienie  mojej  pracy  dyplomowej / rozprawy doktorskiej*.

\vfill

Gliwice, dnia {\selectlanguage{polish}\today}

\vfill

\rule{0.5\textwidth}{0cm}\dotfill 

\rule{0.5\textwidth}{0cm}
\begin{minipage}{0.45\textwidth}
{\begin{center}(podpis)\end{center}}
\end{minipage} 

\vfill

\rule{0.5\textwidth}{0cm}\dotfill 

\rule{0.5\textwidth}{0cm}
\begin{minipage}{0.45\textwidth}
{\begin{center}\rule{0mm}{5mm}(poświadczenie wiarygodności podpisu przez Dziekanat)\end{center}}
\end{minipage}


\vfill

* podkreślić właściwe

 


%%%%%%%%%%%%%%%%%%%%%  
\cleardoublepage

\rule{1cm}{0cm}

\vfill  

\begin{center}
\Large\bfseries Oświadczenie promotora
\end{center}

\vfill

Oświadczam, że praca „\Title” spełnia wymagania formalne pracy dyplomowej inżynierskiej.

\vfill



\vfill

Gliwice, dnia {\selectlanguage{polish}\today}

\rule{0.5\textwidth}{0cm}\dotfill 

\rule{0.5\textwidth}{0cm}
\begin{minipage}{0.45\textwidth}
{\begin{center}(podpis promotora)\end{center}}
\end{minipage} 

\vfill
 
 

\cleardoublepage


%%%%%%%%%%%%%%%%%% Table of contents %%%%%%%%%%%%%%%%%%%%%%
\pagenumbering{Roman}
\pagestyle{onlyPageNumbers}
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{PagesWithoutNumbers}{\value{page}}
\mainmatter
\pagestyle{PageNumbersChapterTitles}

%%%%%%%%%%%%%% body of the thesis %%%%%%%%%%%%%%%%%


\chapter{Introduction}

This chapter presents the problem that the project tries to solve and describes the scope of the thesis. It also declares the focus of each chapter.

\section{Description of the problem}

The invention and propagation of the internet has boosted the ways in which technology impacts
all of us. In this age an increasing amount of our everyday life is digitized and dependent 
on cybernetic systems hosted and operated by independent corporations and institutions. Significant
amount of our tasks has become automated through information technology solutions. The phisical word 
surrounding us also becomes intertwined with technology. We are gradualy connecting things of everyday 
use like cars or house locks to the web through Internet of Things technologies. This process has 
made our lives simpler and allowed us to achieve amazing things but it has also made us vulnerable to 
cybernetic attacks. It is only natural that the rize of the impact of technology was followed 
by the rise in the cyber crime and cyber security providers \cite{bib:articleImportanceOfCybersecurity}. 

Over the years an ecosystem has emerged that constantly competes with malicious hackers to keep us all secure.
One of the elements of this structure is penetration testing also known as ethical hacking. In it's core, 
this practice is simply simulating a real attack. There are multiple sources that depict approaches 
used to perform this process. One of the common denominators between all of them is the importance of 
gathering information \cite{bib:bookEthicalHacking}. The reason for that is because the more information an attacker can uncover and analyze, 
the bigger the chance of finding vulnerable systems or flaws in them. One of the clusters of information 
in companies is a communication channel like slack or discord. There are manny situations where employees 
share information connected to projects and their workplace enviroment. If an attacker was to acces such a 
platform he could potentially analyse the conversation history in search of sensitive information like ip addresses, 
logins, passwords, emails, phone numbers, etc.

Such information is especially important from a legal point of view. Introduction of General Data Protection
Regulation in 2016 has put a preassure on manny legal bodies to responsibly handle people's personal data under
a threat of heavy financial penalties \cite{bib:bookRODO}. Monitoring of the data located in the private entity's internal communication
channel might prove very useful in fulfilling the legislative requirements of private information processing.

Unfortunately such a task may be very time and resource consuming. A tool capable of scanning the history of 
communication channel in search of data that would meet some established criteria could however fulfill this job or at least
increase efficiency of the person responsible for it.

\section{Project scope}

This thesis focuses on the research into proper implementation of a cyber security tool with the purpose of processing 
a large amount of natural language messages in search of data that can be classified as personal or sensitive from the cyber security
point of view. Special focus is put on possibility of calibration and customisation of the search engine and it's reusability, regardless of 
circumstances and enviroment.

The project will primarily focus on finding nine categories of information:

\begin{itemize}
   \item IP addresses - numerical label assigned to each device connected to a computer network that uses the Internet Protocol for communication \cite{bib:articleIP}.
   \item National identification numbers - numbers issued by a Government Entity as a means of providing Identification of their citizens and or Aliens \cite{bib:internetIdentityNumber}.
   In this thesis it is frequently refered to as social security number.
   \item ID card numbers - serial numbers of documents (such as a cards) bearing identifying information about and often a photograph of the individual whose 
   name appears on it \cite{bib:internetID}. 
   \item MAC addresses - unique identifier for an Ethernet or network adapter over a network. It distinguishes different network interfaces and is used for a number 
   of network technologies, particularly most IEEE 802 networks, including Ethernet \cite{bib:internetMAC}.
   \item Domain names - combinations of letters and numbers used in combination of the various domain name extensions, such as .com, .net to find and 
   identify computers on the Internet \cite{bib:internetDomain}.
   \item Email addresses - serieses of letters, numbers, and symbols used to send and receive email \cite{bib:internetEmail}.
   \item Passwords - secret words or combinations of letters or numbers, used for communicating with another person or with a computer to prove who you are \cite{bib:internetPassword}.
   \item Usernames - a unique sequence of characters used by a person with access to a computer, network, or online service \cite{bib:internetUsername}.
   \item Phone numbers - numbers assigned to a telephone line for a specific phones or set of phones (as for a residence) that are used to call those phones \cite{bib:internetPhone}.
   \item Additional - data types specified and implemented by the individual users.
\end{itemize}


\section{Description of chapters}

This document is divided into seven chapters with following focus:

\begin{itemize}
   \item Chapter 1 Introduction - Presentation of the problem domain and scope, introduction of the document structure.
   \item Chapter 2 Problem analysis - Research of the topic and design of the program.
   \item Chapter 3 Requirements and tools - Description of the used technologies and required prerequisites.
   \item Chapter 4 External specification - Instruction of the program usage.
   \item Chapter 5 Internal specification - Elaboration on the program internal structure.
   \item Chapter 6 Verification and validation - Presentation of the testing methodology and results.
   \item Chapter 7 Conclusions - Final remarks and conclusions for the future of the project.
   \end{itemize}

\chapter{Problem analysis}

In this chapter a analysis of solutions tackling similar problems is performed. It looks into methods used in them and checks their validity in the project scope.

\section{Existing solutions}

There are not manny widely available solutions that are capable of performing the job presented in the thesis introduction. However the scope of the project shares
similarities with Data Lekage Prevention Systems which focus on analysis of the content of confidential data and the surrounding context in order to
prevent unwanted disclosures of information. Those programs usually use up to three techniques to monitore the sensitive information - regular expressions (regexes),
data fingerprinting and statistical analysis \cite{bib:articleDLPS}. This approach could be adapted for the purposes of this thesis. 

Regular expressions are an abstraction of key-word search that enables the identification of text using apattern instead of an exact string. They are a tool 
frequently used for parsing users input and capturing parts of strings \cite{bib:conferenceRegex}. Regexes are used in Data Leakage Prevention Systems for detecting 
data like credit card numbers and social security mumbers \cite{bib:articleDLPS} which belong to the scope of this thesis. It might be possible to extend this approach 
in to other categories of information covered by the designed program.

Regular expressions are however known to have high false positives rates \cite{bib:articleDLPS}. It might be advantageous for the purposes of this project 
to take under consideration additional properties of the considered data types. It is possible for the personal information issued
by some entities to implement a check sum algorithm used for validation of authenticity. Implementing a adjustable additional check 
of the data discovered by regular expressions could possibly reduce the amount of false positives. Such feature could be utilised by 
individual user to further enhance accuracy in any case where identifing additional patterns, that escape regular expression domain, is possible.

While dictionary search does not seem to be a common part of Data Leakage Prevention Systems it might prove useful in achieving the goal of this thesis.
The natural language messages processed by the developed program may contain keywords indicating presence of the desired data, which might have not been 
detected by the initial regex search. Implementing a dictionary search which focuses on words like "password", "login" etc. could potentially increase the
reliability of the solution. 

\section{Modularity and Customisation}

Each usecase of the designed system might differ depending on the user, enviroment and multitude of other conditions. 
Some clients of the program might be interested in searching for a subset of data types implemented in the program. It is
also possible that an unanticipated in the design information type might be of a particular interest for a user. 

In order to mitigate those problems a modular approach to the design of program could be taken. Its main goal would be to allow specifing
which of the stock implemented data types to search for as well as simplifying the augmentation of the program's scope.

It is also crucial to take under consideration the fact that the structure of some data types can differ significantly between uses.
An example of this is a personal identification number which may vary depending on issuing authority. Giving the user a easily accessible possibility to 
adjust the regular expressions used in searching process as well as addictional check functions may increase the use cases of the solutions.

\section{Result presentation}

While the focus of this thesis is a program which focuses on finding some sensitive data, it might be profitable to put a special emphasis on the proper
presentation of the results. Information discovered by the solution might be riddled with false positives. In such case giving the user a possibility
to easily analyse the message and conversation context of found data could increase the usefulness of the tool. 

\section{Enviroment independance}

The typical user of the developed program might need to run it on multiple different operating systems (OS). Administrators might require the program to run
on Windows operating system while IT security specialists could possibly want to deploy it on a Kali Linux which is a Debian-based Linux distribution 
aimed at advanced Penetration Testing and Security Auditing. Developing a program capable of runing regardless of the enviroment in which it is used would possibly
allow for a bigger user base.

\section{Input variability}

The program is supposed to be capable of operating on manny different types of input like emails, Facebook messages, Slack messages etc. In order to 
allow for that it may be necessary to design a arbitrary input format. A user would have to be required to transform initial data in their possession to 
fit this predefined standard. 

\chapter{Requirements and tools}

This chapter tackles tools and technologies used during development. It also presents the functional and non-functional requirements of the program.

\section{Tools}
In order to achieve the goal of this thesis following tools were used:
\begin{itemize}
   \item Visual Studio Code - a source-code editor developed by Microsoft for Windows, Linux and macOS. It's source code is free and open source and 
   released under the permissive MIT License. It was used with a Python plugin which provides a rich support for the Python language , including 
   features such as IntelliSense, linting, debugging, code navigation, code formatting, Jupyter notebook support, refactoring, variable explorer, 
   test explorer, snippets, and more \cite{bib:internetVSC}\cite{bib:internetVSCLicence}.
   \item Qt Designer - tool for designing and building graphical user interfaces (GUI) with Qt Widgets. Allows for composing and customization 
   of windows or dialogs in a what-you-see-is-what-you-get (WYSIWYG) manner, and testing of them using different styles and resolutions\cite{bib:internetQt}.
\end{itemize} 
\section{Technologies}
Entire project is developed with use of following technologies:
\begin{itemize}
   \item Python 3.7.3 - an easy to learn, powerful programing language. It has efficient high-level data structures 
   and a simple but effective approach to object-oriented programing. Python’s elegant syntax and dynamic typing, together with its interpreted nature, make it 
   an ideal language for scripting and rapid application development in many areas on most platforms \cite{bib:bookPython}. There are multiple arguments that support
   choosing it as primary development language for the project. It has implemented modules for regular expression search and graphical user interface.
   The scriptive nature of it makes it easy to create programs that are capable of runing in any operating system that has Pyhton interpreter installed.
   It is also familiar to the developer of the program.
   \item JSON - a lightweight data-interchange format. It is easy for humans to read and write. It is easy for machines to parse and generate. It is based on a
   subset of the JavaScript Programing Language Standard ECMA-262 3rd Edition - December 1999. JSON is a text format that is completely language independent but
   uses conventions that are familiar to programers of the C-family of languages, including C, C++, C\#, Java, JavaScript, Perl, Python, and many others. These
   properties make JSON an ideal data-interchange language \cite{bib:internetJSON}.
\end{itemize}

\section{Requirements}

The program designed in this thesis is supposed to perform following tasks:
\begin{itemize}
   \item Perform a regex search on natural language message sets.
   \item Additionally check data found with regular expressions with false positives check function if one is available.
   \item Perform dictionary search on natural language message sets.
   \item Display the found data.
   \item Allow for displaying of message context of found data.
   \item Allow for implementation of custom searches.
   \item Save to and load results from a output file.
\end{itemize}

The program designed in this thesis is supposed to be capable of runing regardless of operating system.

% \section{Use cases}

%%%%%%%%%%%%%%%%UML Diagrams%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{itemize}
% \item functional and nonfunctional requirements
% \item use cases (UML diagrams)
% \item description of tools
% \item methodology of design and implementation
% \end{itemize} 

\chapter{External specification}

Following chapter describes what requirements have to be satisfied for the program usage and how to handle it.
% \begin{itemize}
% \item hardware and software requirements
% \item installation procedure
% \item activation procedure
% \item types of users
% \item user manual
% \item system administration
% \item example of usage
% \item working scenarios (with screenshots or output files)
% \end{itemize}

\section{Requirements and installation}

The project consists of files "program.py" contained in the main program directory and
"codeLib.py", "core.py", "regexLib.json" stored in "src" subdirectory. All of them are 
required for the proper operation of the solution.

For the program to work correctly, some software requirements have to be meet. It was developed with 
Python and requires it's interpreter in version at least 3.7.3 to be installed. Additionally a package called PyQt5 has 
to be be in the Python interpreter library. It can be obtained with package-management 
system called pip. Example of installation script that fulfill the above requirements can be found
in figure \ref{fig:windowsScript} for Windows and figure \ref{fig:linuxScript} for Linux.
The rest of the Python packages used in the project are usually contained within standard Python installation.
In individual cases additional installation may be required.

\begin{figure}
   \centering
   \begin{lstlisting}
   python -m pip install pyqt5
   \end{lstlisting}
   \caption{Windows installation script}
   \label{fig:windowsScript}
\end{figure}

\begin{figure}
   \centering
   \begin{lstlisting}
   #!/bin/bash
   sudo pip3 install PyQt5
   \end{lstlisting}
   \caption{Linux installation script}
   \label{fig:linuxScript}
\end{figure}

The solution was not tested from the hardware requirements point of view however 
during development it was deployed on a computer with following specifications:

\begin{itemize}
   \item Intel Core i5-6300HQ CPU 2.30 GHz
   \item 8GBytes DDR4 RAM
   \item Samsung SSD 860 EVO M.2 500GB
\end{itemize}

\section{User manual and usage examples}

The program requires preparing of the input data in a JSON file containing
a array of objects composed of two key-value pairs. Key "id" needs to be 
suppied with unique id value for each object. Key "content" is supposed to be 
paired with message text, this value will be processed in the search operation 
of the program. Example proper input file is presented in figure \ref{fig:inputFormat}.

\begin{figure}
   \centering
   \begin{lstlisting}
   {
      messages:
      [
         {
            "id":"<unique_id>",
            "content":"<message_content>"
         },...
      ]
   }
   \end{lstlisting}
   \caption{Format of the input JSON file}
   \label{fig:inputFormat}
\end{figure}

Configuration of the program is done partialy with two files contained in it's
file structure. The "regexLib.json" contains the regular expression of a given
data type and a optional name of the method used to to perform additional 
validation. This JSON file is structured in a following maner. It contains
three objects marked with arbitrary keys: "main", "additional", "dictionary".
These represent categories used for different types of search and contain
objects consisting of two key-string pairs. The "regex" key represents regular
expression utilised in the primary search and "code" contains name of the method 
used for additional validity check. The "code" key can be paired with an empty string
if no additional check is required.

The "main" category contains data types marked by following keys:

\begin{itemize}
   \item "ip\_v4" - IP version 4.
   \item "ip\_v6" - IP version 6.
   \item "socialsec" - personal identity numbers.
   \item "id\_number" - ID card numbers.
   \item "mac" - MAC addresses.
   \item "domain" - Domain names.
   \item "email" - Email addresses.
   \item "password" - Passwords.
   \item "login" - Usernames.
   \item "phone\_number" - Phone numbers.
\end{itemize} 

The "additional" object can be filled with any data type desired by the user as long 
as it follows convention from the main category. The keys of each of those objects can have any value and represent the names of categories
specified by the customers.

Finally the "dictionary" object contains regular expressions of words that are supposed to be matched with dictionary search. Each object in this category 
is composed of a single key-value pair. It's key is "regex" and value is the regular expression used in the program. 

When creating regular expressions for the "regexLib.json" files it is important to remember the restrictions of the 
JSON format. Some characters in it might require escaping or representing in UTF-8 notation.

The example "regexLib.json" file can be found in the Listings part of the appendix.

The "codeLib.py" file is intended to contain methods for additional validation. The methods are supposed
to be writen in Python 3, accept two parameters and return either "True" if the validation was succesfull
or "False" if not. First parameter is a string containing the data discovered by the regular expression and
the second is the full content of the message in which it was found.

The example "codeLib.py" file can be found in the Listings part of the appendix.

Starting of the program can be done by runing the "program.py" file with Python interpreter. The proper command
might differ depending on the operating system. Example commands run on a Windows OS from the project directory
can be found in figure \ref{fig:startCommand}.

\begin{figure}
   \centering
   \begin{lstlisting}
   C:\Projects\Thesis\Code> python .\program.py
   \end{lstlisting}
   \caption{Command for starting the program}
   \label{fig:startCommand}
\end{figure}

The main window of the program consists of two tabs. The settings tab allows to setup the search criteria and perform
the desired job. It features three text inputs that allow for specifing files used during program execution.

\begin{itemize}
   \item Input path - specifies location of the input file. It is obligatory for the program execution.
   \item Output path - provides the output location used for saving the results of program execution. 
   If left empty, the program appends the input path with "\_output" suffix. 
   \item Dictionary path - points to the dictionary file. The proper format of a dictionary
   is a txt file filled with all lower case words, separated with new line characters. This path is required
   Only in case of performing a dictionary search.
\end{itemize}

The settings tab allows also to specify what types of data to search for with checkboxes labeled:

\begin{itemize}
   \item IP v4
   \item IP v6
   \item Social Sec no
   \item ID no.
   \item MAC
   \item Domains
   \item Emails
   \item Passwords
   \item Logins
   \item Phone no.
   \item Dictionary
   \item Additional
\end{itemize}

Consequent to specifing the desired data types and required file paths the user can start and stop the search process
with buttons labeled respectively "Start" and "Stop".

Upon completion of the program work results are presented in the "Results" tab. It consists of three columns used for
navigation of the results: 
\begin{itemize}
   \item Category - allows for specification of the search category to browse. Clicking on a given category loads
   the results column.
   \item Results - allows to browse the found results in a given category. Clicking on a given result loads the 
   occurance column.
   \item Occurance - allows to browse the id's of messages in which the result was found. Clicking on a given id
   opens the preview window with the content of the specified message.
\end{itemize}

The goal of the preview window is to inspect the context of a found information. It consists of a central part 
displaying the content of a message and "Previous" and "Next" buttons used for navigation of the messages stored
in the input file, relative to the currently presented message.

It is also possible to load a previously generated output file with the "Open output" option from the
"File" menu located in upper left corner of the main window.

\section{Security}

While the focus of the program is providing additional cyber security and data safety, it's 
improper use could result in security issues. The core of the solution implements
dynamic importing of Python code which under some conditions could be used with malicious
intent for purposes like privilege escalation. It is important to use the program 
responsibly. Proper approach should contain at least restraining from executing
the program as administrative users. It is additionally recomended to be aware of the 
functions stored in src/codeLib.py and of the results of their execution. 

\chapter{Internal specification}

This chapter presents the internal structure of the program. It focuses on the used technologies, algorithms and the system architecture.
% \begin{itemize}
% \item concept of the system
% \item system architecture
% \item description of data structures (and data bases)
% \item components, modules, libraries, resume of important classes (if used)
% \item resume of important algorithms (if used)
% \item details of implementation of selected parts
% \item applied design patterns
% \item UML diagrams
% \end{itemize}


\section{System architecture}

The system consists of two parts, separated into two files, one responsible for graphical user interface and other for
performing the goal of the program. The GUI is implemented in two classes called "Ui\_window" used for the main window of the program and "Ui\_PreviewDialog" responsible for displaying 
the message context preview. The logic of the system is implemented in classes "Parser" and "Core". The first one is used to retrive individual messages from the input file while the second
processes them in search desired data. Additionally a "Signal" class is used to pass messages from the "Core" to the GUI classes. Unified modeling language (UML) diagram presenting the individual classes and connections between them can be found in the figure %%%%%%%UML  

Custom implemented classes utilise following Python packages to perform their tasks: 

\begin{itemize}
   \item PyQt5 - used in GUI implementation.
   \item json - utilised for data input and output.
   \item os - used or enviroment related actions like proper joining of paths.
   \item traceback - required for proper error presentation.
   \item importlib - allows for dynamic loading of the additional check functions.
   \item threading - implements Python multi-thread operations.
   \item re - package for working with Python regular expressions.
\end{itemize}

\section{Algorithms}

The algorithm taking on the main workload of the solution is a regular expression search. In it each character in the text to be searched is examined in sequence against a 
list of all possible current characters. During this examination a new list of all possible next characters is built. When the end of the current list is reached, the new list becomes the current list, the next 
character is obtained, and the process continues. This algorithm continually takes the left derivative of the given regular expression with respect to the text to be searched. The parallel nature of this algorithm 
makes it extremely fast \cite{bib:articleRE}. This solution utilises a Python module that provides regular expression matching operations.

\chapter{Verification and validation}

This chapter describes methodology of testing of the solution and it's results.

\section{Data source}

Data used in the testing of the program was sourced from a real life Facebook account. The history of communication through the platform was downloaded and adapted to the input format with a python script.
The messages were divided into batches of sizes 1000, 2000, 5000, 10000, 20000, 50000, 100000, 227360. The script used for the preparation of data can be found in the Listings section of the appendix.
The owner of the data was of a Polish nationality which had to be taken under consideration during the program execution. Due to the personal nature of the data set it had to be 
redacted and is not a part of this thesis.

\section{Testing methodology}

The entirety of testing was done manualy on a single data set. The effectiveness was tested on a 50000 sample and
the efficiency tests were performed on 227360 message set.

While a context analysis was performed to check if found results are not false positives, it is uncertain if and how manny
positive results were omitted due to the data format unforeseen in regular expression implementation. 

Following regexes were used during the testing: 

\begin{itemize}
   \item IP v4 - "(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\textbackslash .)\{3\}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)" - 
   \item IP v6 - "(?:[0-9a-fA-F]\{1,4\}:)\{7\}[0-9a-fA-F]\{1,4\}"
   \item Social Sec no. - "(?:\textbackslash s|\textbackslash A)[0-9]\{11\}(?:\textbackslash s|\textbackslash Z)"
   \item ID no. - "[A-Za-z]\{3\}[ ]*[0-9]\{6\}"
   \item MAC - "([0-9A-Fa-f]{2}[:-])\{5\}([0-9A-Fa-f]\{2\})"
   \item Domains - "[a-zA-Z0-9][a-zA-Z0-9-]\{1,61\}[a-zA-Z0-9]\textbackslash .[a-zA-Z]\{2,\}"
   \item Emails - "(?:\textbackslash s|\textbackslash A|:|-)([a-zA-Z0-9\_.+-]+@[a-zA-Z0-9-]+\textbackslash .[a-zA-Z0-9-.]+)"
   \item Passwords - "(?:\textbackslash s|\textbackslash A)\textbackslash S\{8,32\}(?:\textbackslash s|\textbackslash Z)"
   \item Logins - "(?:\textbackslash s|\textbackslash A)[a-zA-Z0-9]\{3,7\}(?:\textbackslash s|\textbackslash Z)"
   \item Phone no. - "(?:\textbackslash s|\textbackslash A|:|-)(?:[0-9]\{2\} ?[0-9]\{3\} ?(?:[0-9]\{2\} ?)\{2\}|(?:[0-9]\{3\}[ ]?)\{2\}[0-9]\{3\})(?:\textbackslash s|\textbackslash Z)"
   \item Dictionary - "[0-9a-zA-Zł]\{4,8\}"
\end{itemize}

Their preparation was partialy bassed on the regional information of the data owner like a common structure of phone numbers, format of the social security number in the
country of orgin, etc. Some of them remain true to the international standards like the one used for email addresses. Third group was designed solely from guesses and
some vague standards. This set consists of password and login regular expressions.

Additional check algorithms were implemented for following data types:

\begin{itemize}
   \item Passwords - checks if found word contains a special character, a numerical, a capital letter and a small letter et the same time
   \item Login - checks if found word contains a letter and a number
   \item Social Sec no. - calculates the check number implemented in polish PESEL and checks it's validity.
   \item ID no. - calculates the check number implemented in polish ID number and checks it's validity.
\end{itemize}

The password and login searches are mutually exclusive based on the length of the found string but follow the same
check rules.

Dictionary used in search process consisted of eight keywords presented in figure \ref{fig:dictionary}. 
Some of the words used are translations of the ones originating in english to polish. Their focus was on discovering
additional data in following categories: Emails, Logins, Passwords.

\begin{figure}
   \centering
   \begin{lstlisting}
   login
   haslo
   has\u0142o
   password
   pass
   username
   email
   mail
   \end{lstlisting}
   \caption{Testing dictionary}
   \label{fig:dictionary}
\end{figure}

The additional search was not tested outside of a basic check of proper execution because it's use is completely dependent on the user.

\section{Results}

The results of running all the search categories on a 50000 messages sample are presented in table \ref{id:tab:wyniki}.
Additional analysis of the result showed that multiple found domains were actually subparts of a single
domain. It also happened that discovered domains were actually parts of a email addersses. In those cases
they were rejected as false positives however when they led to an email as well as domain they 
were considered valid results. 

\begin{table}
   \centering
   \caption{Results of the execution on test data}
   \label{id:tab:wyniki}
   \begin{tabular}{rrrr}
   \toprule
      Category       &   Found &   False positives & Positives \\
      IP v4          &      15 &                 1 &        14 \\
      IP v6          &       0 &                 0 &         0 \\
      Social Sec no. &       1 &                 0 &         1 \\
      ID no.         &       6 &                 2 &         4 \\
      MAC            &       0 &                 0 &         0 \\
      Domains        &     516 &                58 &       458 \\
      Emails         &      25 &                 0 &        25 \\
      Passwords      &     105 &               100 &         5 \\
      Logins         &     144 &               143 &         1 \\
      Phone no.      &      41 &                 0 &        41 \\
   \bottomrule
   \end{tabular}
   \end{table}  

The analysis of the dictionary keyword search allowed to identify additional data in some categories. The results can be found in table \ref{id:tab:wynikiDictionary}

\begin{table}
   \centering
   \caption{Dictionary search additional discoveries}
   \label{id:tab:wynikiDictionary}
   \begin{tabular}{rr}
   \toprule
      Category       &   Additional result \\
      Emails         &                   0 \\
      Passwords      &                   4 \\
      Logins         &                   3 \\
   \bottomrule
   \end{tabular}
   \end{table} 

Efficiency of additional checks is presented in table \ref{id:tab:wynikiNoCheck}. Due to the large amount of potential passwords and logins
found in the unchecked search the number of false positives is unknown. Manual context check of this amount of data proved imposible in this 
thesis.

\begin{table}
   \centering
   \caption{Influence of additional check}
   \label{id:tab:wynikiNoCheck}
   \begin{tabular}{rrrrrrr}
   \toprule
                     &           \multicolumn{3}{c}{Additional check}    &    \multicolumn{3}{c}{No check}\\
                                 \cmidrule(r){2-4}                            \cmidrule(r){5-7}
      Category       &   Found &   False positives & Positives &   Found &   False positives & Positives \\
      Social Sec no. &       1 &                 0 &         1 &       2 &                 1 &         1 \\
      ID no.         &       6 &                 2 &         4 &      18 &                16 &         4 \\
      Passwords      &     105 &               100 &         5 &   25748 &              unk. &      unk. \\
      Logins         &     144 &               143 &         1 &   15634 &              unk. &      unk. \\
   \bottomrule
   \end{tabular}
   \end{table} 


The timespan of the program execution on a 227360 samples data set can be found in table \ref{id:tab:wynikiTime}.

\begin{table}
   \centering
   \caption{Timespan of execution}
   \label{id:tab:wynikiTime}
   \begin{tabular}{rr}
   \toprule
      Category       &  Time in seconds \\
      IP v4          &           1.3349 \\
      IP v6          &           0.9224 \\
      Social Sec no. &           0.8259 \\
      ID no.         &           0.9299 \\
      MAC            &           0.8999 \\
      Domains        &           1.2889 \\
      Emails         &           0.9979 \\
      Passwords      &           1.3460 \\
      Logins         &           1.5800 \\
      Phone no.      &           1.0913 \\
      Dictionary     &           1.3450 \\
      All            &           9.3316 \\  
   \bottomrule
   \end{tabular}
   \end{table}  

\chapter{Conclusions}

This chapter is a analysis of the solution, development and testing process. It states the possible future development goals and conclusions on the topic.

\section{Additional tests}

The interaction of the users with the solution was not tested during the development process.
At this time it is uncertain how the necessity of preparing the data will influence the usefulness
of the program. Additional tests should be conducted to analyse how the users interact with the
program to conduct if it is usable. 

In order to definitely conclude how accurate the search settings are a sample data set should be prepared.
In such test sample all quantities of different information types as well as their formats should be known.
Such set would allow to analyse how narrowing down the search scope influences the results.

Additionally the solution was tested only on a data set from a single source. Multiple source 
tests should be conducted to check how different cases impact the usefulness of solution. 

\section{Additional checks}

The possibility of performing additional checks can greatly decrease the amount of false positives.
It's main limitation is the need to identify a pattern or validation function specific to the data catogory.
In the testing this is especially apparent in the search of logins and passwords. Searching for any string 
of characters yelds results practically imposible to analyse. However it can be assumed that a password policy
requires it to consist of arbitrary minimum number of characters, contain capital and small letters and numericals.
Applying these common practices used during their creation significantly improves the output of the program. 
A skilled user could analyse the enviroment of the data source to find additional patterns specific to each use.
An example of such action could be identifing a internal company password policy and applying it in search.
It is also possible to apply more and more precise constrains on the search patterns util reaching a amount of 
results that is feasible to manual processing. 

It is however worth mentioning that a dictionary search of keywords related to passwords and
logins is as or more effective than regular expression check of the data and requires relatively
less preparation to work. Running a keyword search first and implementing a regex search after 
insufficient valiable results were discovered might be a proper approach.

\section{Future development}

The search performed on a dataset of 227360 messages is done in a resonable time. In the future development
spliting of the search to multiple threads, one for each category can be implemented in order to more efficiently
hadle working on extremely large imputs. 

It became apparent during testing that in some cases a single result can present as multiple findings. 
A feature could be implemented to check for reoccuurrence of a found data to improve
results presentation. The processing of the output could also be augmented with a classification
option that would assign priority to some results based on set weights, results of the additional
check function and other factors.
 
Usage of Python proved very useful in prototyping of the solution. It did however come with some difficulties.
The code of the soultion is lacking some coding practices and became difficult to implement changes.
The graphical user interface framework used, at times, was unable to provide the desired functionality. This led to some
compromises in the development. In the future a refactoring of solution into a different programing language might 
be necessary in order to continue development. 

Currently the preview functionality used for displaying of found data and message context 
does not allow to perform any changes on the results. A possibility of removing false positive 
results and performing changes on the found snippets of text could greatly improve the 
usefulness of the program in it's working scenarios. This would require implementation of a saving of the work
done to the output file.

In a case where adapting data to the format required for the program poroves to be a significant 
obstruction for the program users, a modular input adapter capable of accepting dataset common in 
real use cases could be developed. An example of that could be a possibility to read mbox files 
used to archive emails or loading dumps of slack messages.

The use of a json file to store the regular expressions might be uncomfortable for the users due to the need of escaping characters. 
This problem could be amended by implementing a different storing mechanism or a tool to manipulate the regexes in their natural form 
and writing them in the transformed form to the file. 

\section{Calibration}

The process of calibration of the program may be complicated and dependent on manny factors. It might be 
useful to develop a calibration framework and create a manual for development of search constrains.



 

 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\backmatter
\pagenumbering{Roman}
\stepcounter{PagesWithoutNumbers}
\setcounter{page}{\value{PagesWithoutNumbers}}

\pagestyle{onlyPageNumbers}

%%%%%%%%%%% bibliography %%%%%%%%%%%%
\bibliographystyle{plain}
\bibliography{bibliography}

%%%%%%%%%  appendices %%%%%%%%%%%%%%%%%%% 

\begin{appendices} 


 

\chapter*{List of abbreviations and symbols}

\begin{itemize}
\item[OS] operating system
\item[regex] regular expression
\item[GUI] Graphical user interface
\item[UML] Unified modeling language 
\end{itemize}


\chapter*{Listings}

   
Example of a "regexLib.json" file:

\begin{lstlisting}
{
   "main":{
         "ip_v4": {
            "regex":"(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)",
            "code":""
         },
         "ip_v6": {
            "regex":"(?:[0-9a-fA-F]{1,4}:){7}[0-9a-fA-F]{1,4}",
            "code":""
         },
         "socialsec":{
            "regex":"(?:\\s|\\A)[0-9]{11}(?:\\s|\\Z)"
            ,"code":"pesel"
         },
         "id_number":{
            "regex":"[A-Za-z]{3}[ ]*[0-9]{6}",
            "code":"idnum"
         },
         "mac":{
            "regex":"([0-9A-Fa-f]{2}[:-]){5}([0-9A-Fa-f]{2})",
            "code":""
         },
         "domain":{
            "regex":"[a-zA-Z0-9][a-zA-Z0-9-]{1,61}[a-zA-Z0-9]\\.[a-zA-Z]{2,}",
            "code":""
         },
         "email":{
            "regex":"(?:\\s|\\A|:|-)([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)",
            "code":""
         },
         "password":{
            "regex":"(?:\\s|\\A)\\S{8,32}(?:\\s|\\Z)",
            "code":"passwd"
         },
         "login":{
            "regex":"",
            "code":""
         },
         "phone_number":{
            "regex":"(?:\\s|\\A|:|-)(?:[0-9]{2} ?[0-9]{3} ?(?:[0-9]{2} ?){2}|(?:[0-9]{3}[ ]?){2}[0-9]{3})(?:\\s|\\Z)",
            "code":""
         }
   },
   "additional":{
         "l":{
            "regex": "[\u0142]{1}",
            "code": ""
         },
         "\u0119":{
            "regex": "\u0119",
            "code": ""
         }
   },
   "dictionary":{
         "dictionary": {
            "regex": "[0-9a-zA-Z\u0142\u0119]{3,8}"
         }
   }
}
\end{lstlisting}

Example of a "codeLib.py" file:

\begin{lstlisting}
def pesel(snip, content):
   snip = snip[1:-1]
   weight = [9, 7, 3, 1, 9, 7, 3, 1, 9, 7]
   checksum = (int(snip[0]) * weight[0]) + (int(snip[1]) * weight[1]) + (int(snip[2]) * weight[2]) + (int(snip[3]) * weight[3])+ (int(snip[4]) * weight[4]) + \
       (int(snip[5]) * weight[5]) + (int(snip[6]) * weight[6]) + (int(snip[7]) * weight[7]) + (int(snip[8]) * weight[8]) + (int(snip[9]) * weight[9])

   if checksum%10 == int(snip[-1]):
       return True
   else:
       return False

def idnum(snip, content):
   weight = [7, 3, 1, 0, 7, 3, 1, 7, 3]
   key = {"A": 10, "B": 11, "C": 12, "D": 13, "E": 14, "F": 15, "G": 16, "H": 17, "I": 18, "J": 19, "K": 20, "L": 21, "M": 22, \
       "N": 23, "O": 24, "P": 25, "Q": 26, "R": 27, "S": 28, "T": 29, "U": 30, "V": 31, "W": 32, "X": 33, "Y": 34, "Z": 35}

   snip_ready = snip.replace(" ", "").upper()
   checksum = (key[snip_ready[0]] * weight[0]) + (key[snip_ready[1]] * weight[1]) + (key[snip_ready[2]] * weight[2]) + (int(snip_ready[3]) * weight[3]) + \
       (int(snip_ready[4]) * weight[4]) + (int(snip_ready[5]) * weight[5]) + (int(snip_ready[6]) * weight[6]) + (int(snip_ready[7]) * weight[7]) + (int(snip_ready[8]) * weight[8])

   if checksum%10 == int(snip_ready[3]):
       return True
   else:
       return False

def passwd(snip, content):
   specialChars = "!@#$%^&*()_+-={}[];:\"'\\|<>?,./"
   nums = "0123456789"
   uppers = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
   lowers = "abcdefghijklmnopqrstuvwxyz"
   containsSpecial = False
   containsNum = False
   containsChars = False
   containsLow = False
   containsUpper = False

   for num in nums:
       if num in snip:
           containsNum = True
           break

   if not containsNum:
       return False

   for char in specialChars:
       if char in snip:
           containsSpecial = True
           break
   
   if not containsSpecial:
       return False
   
   for low in lowers:
       if low in snip:
           containsLow = True
           break

   if not containsLow:
       return False

   for up in uppers:
       if up in snip:
           containsUpper = True
           break

   if not containsUpper:
       return False
   else:
       return True
   
\end{lstlisting}

\chapter*{Contents of attached CD}

The thesis is accompanied by a CD containing:
\begin{itemize}
\item thesis (\LaTeX\ source files and final \texttt{pdf} file),
\item source code of the application
\end{itemize}
 

\listoffigures
\listoftables
	
\end{appendices}


\end{document}


%% Finis coronat opus.